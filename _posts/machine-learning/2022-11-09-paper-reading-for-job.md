---
layout: post
title: å·¥ä½œå‰çš„è®ºæ–‡ä»¥åŠä»£ç é˜…è¯» 
---

## å¼ºåŒ–å­¦ä¹ 

* **Top-ğ¾ Off-Policy Correction for a REINFORCE Recommender System**:
  * Difference between on-policy and off-policy:
    * Reinforcement Learning: An Introduction
  * Value-based and policy-based
  * log-trick in REINFORCE gradient calculation[[^1]]:To solve such optimization problems, we have to take the derivative of the expectation, which is not as easy as in the case where we optimize empirical loss instead of the true expected loss because we do not know the true data distribution. In the previous case, the parameter we want to optimize does not depend on the sample. However, in the latter, the dependency of the loss and the parameter is only through the samples, once we sample, the loss doesnâ€™t depend on the parameter anymore. Therefore, we canâ€™t just use empirical estimation and get around. Here we have to really deal with the expectation.(çœ‹ä¸æ‡‚) ä¸èƒ½ä½¿ç”¨é‡å‚æ•°åŒ–æŠ€å·§å—ï¼Ÿ
  * å…¬å¼ä¸‰
  * sampled softmax
  * temperature in softmax(âœ”)
  * why we should reduce gradient variance?[[^3], [^4]]:
  * weight capping:å¥½åƒæ˜¯éè´Ÿçš„å§
  * Boltzmann exploration
  * actor-critic[[^4]]:å‚è€ƒé“¾æ¥é‡Œçš„å†…å®¹æ²¡çœ‹å®Œ![actor_critc]({{site.baseurl}}/images/rf/actor_critic.png)
  * q-learning[[^2]]
  * reinforceï¼š
  * behavior policy $\beta$ ï¼Œitem embeddingå¯ä»¥è®­ç»ƒï¼Œä½†æ˜¯ï¼Œuser stateé‚£é‡Œæ¢¯åº¦ä¸å›ä¼ çš„è¯ï¼Œuser stateä¸€ç›´å˜åŒ–ï¼Œä½†æ˜¯item embeddingæ˜¯ä¸å˜åŒ–çš„ï¼Ÿï¼Ÿï¼Ÿï¼Ÿï¼Ÿ
  * gumbelï¼ˆsoftmaxé‡‡æ ·ï¼‰[[^5]]ï¼šå¤æ‚åº¦å¯ä»¥é™ä½åˆ°O(n).
  * ç¦»æ•£å˜é‡é‡‡æ ·[[^6]]:æœ‰O(1)çš„æ¬¸ï¼Œç›´æ¥å°±æ˜¯ç”Ÿæˆä¸€ä¸ªarrayï¼Œå…¶ä¸­æ•°å€¼åˆ†å¸ƒæŒ‰æ¦‚ç‡åˆ†å¸ƒæ¥ï¼Œç„¶åéšæœºé€‰ä¸€ä¸ªå°±å¥½äº†ã€‚
  * è¿™é‡Œè‚¯å®šæ²¡æœ‰çœ‹å®Œçš„ï¼Œä½†æ˜¯æˆ‘ä¹Ÿä¸çŸ¥é“çœ‹åˆ°å“ªé‡Œäº†ã€‚ã€‚ã€‚ã€‚

* **Deep Reinforcement Learning: Pong from Pixels**[[^7]]:è¿™ä¸ªæ˜¯ç›´æ¥å¯¹æ¦‚ç‡è¿›è¡Œæ›´æ–°ï¼Œä½¿é‡‡æ ·çš„æ¦‚ç‡æ›´åå‘äºæ­£å‘çš„ã€‚ä½†æ˜¯æ²¡æœ‰rewardä¼°è®¡çš„éƒ¨åˆ†ã€‚   

* [REINFORCE](https://github.com/pytorch/examples/blob/d304b0d4a20d97e3b4529cfd6429102a58e7635a/reinforcement_learning/reinforce.py):ç¬¬ä¸€æ¬¡çœ‹åˆ°`itertools.count()`,è¿˜æŒºå¥½ç©çš„ï¼Œå¯ä»¥ç”¨è¿™ä¸ªå†™`while True`.

## word2vec
**code reading** [[^8]]
1. wordé‡‡ç”¨æœ‰ä¸€ä¸ªå¾ˆæœ‰æ„æ€çš„ä¸œè¥¿ï¼Œå°±æ˜¯ï¼Œå¦‚ä½•å¿«é€Ÿé‡‡æ ·å‘¢ï¼Ÿè§£å†³æ–¹æ¡ˆå°±æ˜¯ç”Ÿæˆä¸€ä¸ªéå¸¸å¤§çš„è¡¨é‡Œé¢æŒ‰ç…§æ¦‚ç‡æ”¾è¯æ±‡ç„¶åå°±èƒ½$O(1)$å¯¹wordè¿›è¡Œé‡‡æ ·äº†ã€‚è¿ç»­çš„åŒºé—´ä¹Ÿèƒ½è¿™æ ·å¤„ç†ï¼ï¼ï¼
2. haffmanæ ‘å’Œè´Ÿé‡‡æ ·æ˜¯ä¸€ä¸ªèƒ½ç›¸äº’æ›¿ä»£çš„å…³ç³»ï¼ï¼åŸå§‹ä»£ç ä¸­ï¼Œä¸¤è€…ä¸åŒæ—¶ä½¿ç”¨ã€‚
3. é‡Œé¢éšæœºæ•°ç”Ÿæˆç«Ÿç„¶æ˜¯è‡ªå·±ç”Ÿæˆçš„ï¼Œå°±æ˜¯åˆå§‹ä¸€ä¸ªæ•°ï¼Œç„¶ååå¤ä¹˜ä»¥ä¸€ä¸ªéå¸¸å¤§çš„æ•°å­—ï¼Œå†å–æœ€ä½16ä½åå–å€¼ï¼Œé‡æ–°åå¤ä¹˜å¾—åˆ°æ–°çš„æ•°å­—ã€‚
4. æ¢¯åº¦æ˜¯è‡ªå·±æ‰‹åŠ¨ç®—çš„å‘€ï¼Œéš¾æ€ªè®ºæ–‡è¿™ä¹ˆå¼ºè°ƒæ¢¯åº¦è¿™ä¸ªä¸œè¥¿ã€‚
5. è¿˜æœ‰æ¿€æ´»å‡½æ•°æ˜¯é¢„å…ˆç®—å¥½ä¸¢åˆ°è¡¨é‡Œï¼Œç„¶åé€šè¿‡intè®¿é—®
   1. TODO: ä½†æ˜¯å‘¢ï¼Œ
6. TODOï¼šæˆ‘æ˜¯ä¸æ‡‚skip-gramå®ç°ä¸Šçš„é—®é¢˜ã€‚
7. TODOï¼šæ²¡ä»”ç»†çœ‹å¦‚ä½•æ›´æ–°å‚æ•°çš„ã€‚
   

## å‚è€ƒæ–‡çŒ®

[^1]: [Policy Gradients and Log Derivative Trick](https://medium.com/@aminamollaysa/policy-gradients-and-log-derivative-trick-4aad962e43e0)
[^2]: [An introduction to Q-Learning: Reinforcement Learning](https://blog.floydhub.com/an-introduction-to-q-learning-reinforcement-learning/)
[^3]: [Jerry Liuâ€™s post to â€œWhy does the policy gradient method have high varianceâ€](https://www.quora.com/unanswered/Why-does-the-policy-gradient-method-have-a-high-variance)
[^4]: [Understanding Actor Critic Methods and A2C](https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f)
[^5]: [Gumbel-max trick](https://timvieira.github.io/blog/post/2014/07/31/gumbel-max-trick/)
[^6]: [How to sample from a discrete distribution?](https://stats.stackexchange.com/questions/67911/how-to-sample-from-a-discrete-distribution)
[^7]: [Deep Reinforcement Learning: Pong from Pixels](http://karpathy.github.io/2016/05/31/rl/)
[^8]: [word2vec](https://github.com/catqaq/NLP-Notes/blob/master/word2vec_code/word2vec.c)