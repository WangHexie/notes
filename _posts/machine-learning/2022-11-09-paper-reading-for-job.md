---
layout: post
title: å·¥ä½œå‰çš„è®ºæ–‡ä»¥åŠä»£ç é˜…è¯» 
---

## å¼ºåŒ–å­¦ä¹ 

* **Top-ğ¾ Off-Policy Correction for a REINFORCE Recommender System**:
  * Difference between on-policy and off-policy:
    * Reinforcement Learning: An Introduction
  * Value-based and policy-based
  * log-trick in REINFORCE gradient calculation[[^1]]:To solve such optimization problems, we have to take the derivative of the expectation, which is not as easy as in the case where we optimize empirical loss instead of the true expected loss because we do not know the true data distribution. In the previous case, the parameter we want to optimize does not depend on the sample. However, in the latter, the dependency of the loss and the parameter is only through the samples, once we sample, the loss doesnâ€™t depend on the parameter anymore. Therefore, we canâ€™t just use empirical estimation and get around. Here we have to really deal with the expectation.(çœ‹ä¸æ‡‚) ä¸èƒ½ä½¿ç”¨é‡å‚æ•°åŒ–æŠ€å·§å—ï¼Ÿä»€ä¹ˆé¬¼ä¸œè¥¿ï¼Œæˆ‘çœ‹å®Œå¼ºåŒ–å­¦ä¹ é‚£æœ¬ä¹¦ä¹‹åï¼Œæˆ‘è§‰å¾—log-trickåªæ˜¯ä¸ºäº†å°†æœŸæœ›å…¬å¼å˜æˆå¯è®­ç»ƒçš„ç›¸åŠ å½¢å¼ï¼Œå°†å…¶å±•å¼€å¹¶ä¸”å˜å¾—æ›´åŠ ç®€å•äº†ã€‚
  * å…¬å¼ä¸‰ï¼š è¿™ä¸ªå°±æ˜¯off-policyçš„ä¸€ä¸ªç»å…¸ç­–ç•¥å§ï¼Œæ²¡ä»€ä¹ˆå¥½è¯´çš„ï¼Œå°†æ¦‚ç‡å€¼æ›´æ­£äº†ï¼Œå±•å¼€å°±æ˜¯äº†ã€‚å“¦ï¼Œä¸»è¦æ˜¯ä¸ºä»€ä¹ˆè¦ç”¨åˆ°æœªæ¥çš„tï¼Œä¸»è¦æ˜¯å› ä¸ºRçš„è®¡ç®—ä¸­ç”¨åˆ°äº†æœªæ¥çš„rewardå§ï¼Œæˆ‘è§‰å¾—æ˜¯ï¼Œä½†æ˜¯æ˜¯ä¸æ˜¯è¿™æ ·çš„ï¼Œéœ€è¦è‡ªè¡Œæ¨å…¬å¼çš„ã€‚
  * sampled softmax
  * temperature in softmax(âœ”)
  * why we should reduce gradient variance?[[^3], [^4]]:ä¸ºäº†ä½¿è®­ç»ƒæ›´åŠ ç¨³å®šï¼Œé˜²æ­¢æ¢¯åº¦è¿‡å¤§æˆ–è¿‡å°
  * ä¸ºäº†ç»§ç»­å‡å°‘gradient variance å°†correction è¿ä¹˜çš„é¡¹æ•°å‡å°‘äº†ï¼Œè™½ç„¶æ˜¯æœ‰åçš„ï¼Œä½†æ˜¯ä½œè€…è§‰å¾—æ— æ‰€è°“äº†ã€‚ä½†æ˜¯å‘¢ï¼Œä½ è¿™é‡Œå¹¶æ²¡æ³•å®šä¹‰ç”¨æˆ·
  * weight capping:å¥½åƒæ˜¯éè´Ÿçš„å§
  * Boltzmann exploration
  * actor-critic[[^4]]:å‚è€ƒé“¾æ¥é‡Œçš„å†…å®¹æ²¡çœ‹å®Œ![actor_critc]({{site.baseurl}}/images/rf/actor_critic.png)
  * q-learning[[^2]]
  * reinforceï¼š
  * behavior policy $\beta$ ï¼Œitem embeddingå¯ä»¥è®­ç»ƒï¼Œä½†æ˜¯ï¼Œuser stateé‚£é‡Œæ¢¯åº¦ä¸å›ä¼ çš„è¯ï¼Œuser stateä¸€ç›´å˜åŒ–ï¼Œä½†æ˜¯item embeddingæ˜¯ä¸å˜åŒ–çš„ï¼Ÿï¼Ÿï¼Ÿï¼Ÿï¼Ÿï¼šåªå­¦ä¸€ä¸ª$\theta$ï¼Œè¿™ä¸ªå¯ä»¥å½“ä½œitem embeddingï¼Œä½†æ˜¯ä¸ä¼šç”¨çš„ã€‚
  * gumbelï¼ˆsoftmaxé‡‡æ ·ï¼‰[[^5]]ï¼šå¤æ‚åº¦å¯ä»¥é™ä½åˆ°O(n).
  * ç¦»æ•£å˜é‡é‡‡æ ·[[^6]]:æœ‰O(1)çš„æ¬¸ï¼Œç›´æ¥å°±æ˜¯ç”Ÿæˆä¸€ä¸ªarrayï¼Œå…¶ä¸­æ•°å€¼åˆ†å¸ƒæŒ‰æ¦‚ç‡åˆ†å¸ƒæ¥ï¼Œç„¶åéšæœºé€‰ä¸€ä¸ªå°±å¥½äº†ã€‚
  * è¿™é‡Œè‚¯å®šæ²¡æœ‰çœ‹å®Œçš„ï¼Œä½†æ˜¯æˆ‘ä¹Ÿä¸çŸ¥é“çœ‹åˆ°å“ªé‡Œäº†ã€‚ã€‚ã€‚ã€‚
  * è¿˜æœ‰ä¸€ä¸ªæ˜¯behaviour policyçš„è·å¾—ï¼Œä¸ºäº†è·å¾—behavior policyï¼Œç›´æ¥ä½¿ç”¨äº†åŸå§‹ç”Ÿæˆçš„æ‰€æœ‰æ¨èç‰©å“ï¼Œæ‰€ä»¥å³ä¾¿æœ‰å¤šä¸ªagentæ¥ç”Ÿæˆæ¨èçš„åºåˆ—ä¹Ÿèƒ½å°½é‡å»æ¨¡æ‹Ÿã€‚è€Œä¸”å› ä¸ºç”Ÿæˆçš„æ˜¯behavior policyå¹¶ä¸éœ€è¦å¯¹ç‚¹å‡»ç‡ä»€ä¹ˆçš„è¿›è¡Œé¢„ä¼°ï¼Œæ‰€ä»¥æŠŠæ‰€æœ‰çš„iteméƒ½æ˜¯éƒ½ä¸¢è¿›å»è®­ç»ƒï¼Œè€Œaction policyä½¿ç”¨æœ‰rewardçš„ç‚¹å‡»è¿›è¡Œè®­ç»ƒå‘¢ï¼Œ å³ä¾¿å°†æ‰€æœ‰çš„ä¸œè¥¿ä¸¢è¿›å»ï¼Œå› ä¸ºrewardæ˜¯é›¶ï¼Œæ‰€ä»¥å…¶å®ä¹Ÿå¹¶ä¸æ›´æ–°çš„ã€‚

* **Deep Reinforcement Learning: Pong from Pixels**[[^7]]:è¿™ä¸ªæ˜¯ç›´æ¥å¯¹æ¦‚ç‡è¿›è¡Œæ›´æ–°ï¼Œä½¿é‡‡æ ·çš„æ¦‚ç‡æ›´åå‘äºæ­£å‘çš„ã€‚ä½†æ˜¯æ²¡æœ‰rewardä¼°è®¡çš„éƒ¨åˆ†ã€‚   

* [REINFORCE](https://github.com/pytorch/examples/blob/d304b0d4a20d97e3b4529cfd6429102a58e7635a/reinforcement_learning/reinforce.py):ç¬¬ä¸€æ¬¡çœ‹åˆ°`itertools.count()`,è¿˜æŒºå¥½ç©çš„ï¼Œå¯ä»¥ç”¨è¿™ä¸ªå†™`while True`.

## word2vec
**code reading** [[^8]]
1. wordé‡‡ç”¨æœ‰ä¸€ä¸ªå¾ˆæœ‰æ„æ€çš„ä¸œè¥¿ï¼Œå°±æ˜¯ï¼Œå¦‚ä½•å¿«é€Ÿé‡‡æ ·å‘¢ï¼Ÿè§£å†³æ–¹æ¡ˆå°±æ˜¯ç”Ÿæˆä¸€ä¸ªéå¸¸å¤§çš„è¡¨é‡Œé¢æŒ‰ç…§æ¦‚ç‡æ”¾è¯æ±‡ç„¶åå°±èƒ½$O(1)$å¯¹wordè¿›è¡Œé‡‡æ ·äº†ã€‚è¿ç»­çš„åŒºé—´ä¹Ÿèƒ½è¿™æ ·å¤„ç†ï¼ï¼ï¼
2. haffmanæ ‘å’Œè´Ÿé‡‡æ ·æ˜¯ä¸€ä¸ªèƒ½ç›¸äº’æ›¿ä»£çš„å…³ç³»ï¼ï¼åŸå§‹ä»£ç ä¸­ï¼Œä¸¤è€…ä¸åŒæ—¶ä½¿ç”¨ã€‚
3. é‡Œé¢éšæœºæ•°ç”Ÿæˆç«Ÿç„¶æ˜¯è‡ªå·±ç”Ÿæˆçš„ï¼Œå°±æ˜¯åˆå§‹ä¸€ä¸ªæ•°ï¼Œç„¶ååå¤ä¹˜ä»¥ä¸€ä¸ªéå¸¸å¤§çš„æ•°å­—ï¼Œå†å–æœ€ä½16ä½åå–å€¼ï¼Œé‡æ–°åå¤ä¹˜å¾—åˆ°æ–°çš„æ•°å­—ã€‚
4. æ¢¯åº¦æ˜¯è‡ªå·±æ‰‹åŠ¨ç®—çš„å‘€ï¼Œéš¾æ€ªè®ºæ–‡è¿™ä¹ˆå¼ºè°ƒæ¢¯åº¦è¿™ä¸ªä¸œè¥¿ã€‚
5. è¿˜æœ‰æ¿€æ´»å‡½æ•°æ˜¯é¢„å…ˆç®—å¥½ä¸¢åˆ°è¡¨é‡Œï¼Œç„¶åé€šè¿‡intè®¿é—®
   1. TODO: ä½†æ˜¯å‘¢ï¼Œ
6. TODOï¼šæˆ‘æ˜¯ä¸æ‡‚skip-gramå®ç°ä¸Šçš„é—®é¢˜ã€‚
7. TODOï¼šæ²¡ä»”ç»†çœ‹å¦‚ä½•æ›´æ–°å‚æ•°çš„ã€‚
   

## å‚è€ƒæ–‡çŒ®

[^1]: [Policy Gradients and Log Derivative Trick](https://medium.com/@aminamollaysa/policy-gradients-and-log-derivative-trick-4aad962e43e0)
[^2]: [An introduction to Q-Learning: Reinforcement Learning](https://blog.floydhub.com/an-introduction-to-q-learning-reinforcement-learning/)
[^3]: [Jerry Liuâ€™s post to â€œWhy does the policy gradient method have high varianceâ€](https://www.quora.com/unanswered/Why-does-the-policy-gradient-method-have-a-high-variance)
[^4]: [Understanding Actor Critic Methods and A2C](https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f)
[^5]: [Gumbel-max trick](https://timvieira.github.io/blog/post/2014/07/31/gumbel-max-trick/)
[^6]: [How to sample from a discrete distribution?](https://stats.stackexchange.com/questions/67911/how-to-sample-from-a-discrete-distribution)
[^7]: [Deep Reinforcement Learning: Pong from Pixels](http://karpathy.github.io/2016/05/31/rl/)
[^8]: [word2vec](https://github.com/catqaq/NLP-Notes/blob/master/word2vec_code/word2vec.c)