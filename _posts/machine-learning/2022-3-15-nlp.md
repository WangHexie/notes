---
layout: post
title: nlp面试整理
---

1. BERT:

   1. 别忘了Q*K还要除以$$\sqrt{d_k}$$

   $$
   \operatorname{Attention}(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V
   $$

   2. 别忘了Position embedding和token embedding 还有一个segment embedding
   3. 训练任务是MLM，cls，还有nsp。

2. Transformer：

   1. Decoder里会接两个Attention。先是自己和自己的attention，第二个Attention里$Q$和$$K$$是来自Encoder的输出。

   2. multiple head attention.

      

