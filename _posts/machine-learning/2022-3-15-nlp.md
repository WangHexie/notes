---
layout: post
title: 面试整理
---

1. BERT:

   1. 别忘了Q*K还要除以$$\sqrt{d_k}$$

   $$
   \operatorname{Attention}(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V
   $$

   2. 别忘了Position embedding和token embedding 还有一个segment embedding
   3. 训练任务是MLM，cls，还有nsp。

2. Transformer：

   1. Decoder里会接两个Attention。先是自己和自己的attention，第二个Attention里$K$和$$V$$是来自Encoder的输出。

   2. multiple head attention.[实现](https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/master/transformer/SubLayers.py)

3. 推荐系统中为什么AUC这个指标不是很好(会识别重要用户，但是对线上没什么用)。如何解决？：换NDCG等分组的指标


      

