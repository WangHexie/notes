---
layout: post
title: LDA
---

## LDA [[^1]]
文本集合的概率模型，文本-话题，话题-单词矩阵的多项式概率参数由狄利克雷分布生成，但是仅生成一次，并不是没生成一个单词就生成一个分布，所以可以计算。和逻辑回归的参数可以假设符合正态分布类似，同时该假设与l2正则等价。参数符合预设的分布能降低过拟合概率。


  * 定义：![LDA]({{site.baseurl}}/images/lda/lda_graph.png)

    

  * 概率计算：LDA假设文本由无限可交换的话题序列组成。

    $$
    \begin{equation}
    p(\mathbf{w}, \mathbf{z}, \theta, \varphi \mid \alpha, \beta)=\prod_{k=1}^{K} p\left(\varphi_{k} \mid \beta\right) \prod_{m=1}^{M} p\left(\theta_{m} \mid \alpha\right) \prod_{n=1}^{N_{m}} p\left(z_{m n} \mid \theta_{m}\right) p\left(w_{m n} \mid z_{m n}, \varphi\right)
    \end{equation}
    $$

  * 学习算法：
    *  狄利克雷分布有一些重要性质: (1)狄利克雷分布属于指数分布族; (2)狄利克雷分布是多项分布的共轭先验(conjugate prior)。
       *  狄利克雷分布的后验分布的参数等于先验分布参数加上统计计数。(既然有观察到一定的数据，说明就不是先验分布的概率分布了，其概率分布是与观察到的数据相关的)
    * 吉布斯抽样算法：（从吉布斯抽样到具体算法，how????）
      * 具体的话就是
        1. 根据平均的多项分布随机为每个词分配话题，然后分别增加话题-词语，话题-文本的统计数据。直到所有词语分配话题完成。
        2. 然后根据统计数据，计算隐变量，重新概率，然后重新分配话题，直到燃烧期结束。
    * 变分EM算法：这个完全不懂了。 

      


## 参考文献
[^1]: 统计学习方法(第二版)